{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dimension Reduction for Mixed Data (numerical and categorical) and K-Medoids Clustering with Gower Distance \n",
        "\n",
        "Traditional dimension reduction and clustering techniques cannot be applied directly to categorical data, because the notion of distance measure is unclear for these data. Most reduction and clustering techniques rely on measuring the closeness of observations (or usage of some distance metric in the algorithm), which cannot directly translate to categorical data. You cannot measure the distance between gender, nor color. At least not with any traditional approach.\n",
        "\n",
        "Luckily, researchers have found (multiple) solutions to this problem. I will be using the well known housing data set on Kaggle. This is a simple data set, but it serves the purpose of this project. The details of this data set can be found [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview).\n",
        "\n",
        "I have removed some of the columns that contains too many missing values. I have also removed the remaining missing values. The goal again is to predict the housing price using the predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LqHD1gDZD9Gn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from prince import MCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "import gower\n",
        "from pyclustering.cluster.kmedoids import kmedoids\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the [housing dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview), sourced from Kaggle.\n",
        "\n",
        "Doing a little bit of data preparation for the oncoming analysis:\n",
        "* All features containing more that 30 missing observations are dropped.\n",
        "* All missing values left are dropped, also.\n",
        "* _Sale Price_ is the dependent variable $y$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-5F0ROq0D9Gp"
      },
      "outputs": [],
      "source": [
        "# Load data set\n",
        "data = pd.read_csv('train.csv')\n",
        "data = data.drop('Id', axis = 1)\n",
        "\n",
        "# Remove columns that have too many missing values\n",
        "data = data.drop(data.columns[data.isnull().sum() > 30], axis = 1)\n",
        "\n",
        "# Remove missing values\n",
        "data.dropna(inplace = True)\n",
        "\n",
        "X = data.drop('SalePrice', axis=1)\n",
        "y = data.SalePrice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qRRSPqQD9Gq"
      },
      "source": [
        "## Part I: Dimension Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this first part, we will compare two regression models: a simple linear regression with PCA and MCA dimension reduction, and a Ridge regression. PCA will be used for the numerical features and MCA for the categorical ones.\n",
        "\n",
        "Here's a roadmap of the plan:\n",
        "1. Split the data into 3 pieces (training, validation, testing).\n",
        "2. Make sure the categorical features on the training and validation sets are the same.\n",
        "1. Tune the model.\n",
        "2. Reapply the code, but on the training + validation sets combined, and the test set.\n",
        "3. Fit the final model, perform prediction.\n",
        "\n",
        "First, let's split the dataset into train, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HOLD OUT SET\n",
        "# ------------\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Linear Regression with PCA and MCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To split the dataset by data type, we first identify what types we have on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "int64      33\n",
              "object     28\n",
              "float64     1\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.dtypes.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we proceed with separating these features for both the training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separating categorical and numerical features\n",
        "X_train_cat = X_train.select_dtypes(include=['object'])\n",
        "X_train_num = X_train.select_dtypes(include=['int', 'float'])\n",
        "\n",
        "# For valid test\n",
        "X_valid_cat = X_valid.select_dtypes(include=['object'])\n",
        "X_valid_num = X_valid.select_dtypes(include=['int', 'float'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before applying any MCA fitting and transform, we make sure that each categorical variables in the training set and validation set have the same set of levels. We'll remove any columns that donâ€™t match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validating that the training feature and testing feature have same number of levels\n",
        "keep = X_train_cat.nunique() == X_valid_cat.nunique()\n",
        "# Removing columns whose levels don't match\n",
        "X_train_cat = X_train_cat[X_train_cat.columns[keep]]\n",
        "X_valid_cat = X_valid_cat[X_valid_cat.columns[keep]]\n",
        "\n",
        "# Making sure the classes are the same for categorical features that have same levels\n",
        "keep = []\n",
        "for i in range(X_train_cat.shape[1]):\n",
        "    keep.append(all(np.sort(X_train_cat.iloc[:,i].unique()) == np.sort(X_valid_cat.iloc[:,i].unique())))\n",
        "X_train_cat = X_train_cat[X_train_cat.columns[keep]]\n",
        "X_valid_cat = X_valid_cat[X_valid_cat.columns[keep]]\n",
        "\n",
        "X_valid = pd.concat([X_valid_cat, X_valid_num], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### _Training_\n",
        "\n",
        "Searching for the best model using the training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'pca_comp': 2, 'mca_comp': 17}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the parameter grid\n",
        "n_components = [2, 7, 12, 17]\n",
        "\n",
        "# Define variables to keep track of best parameter and score\n",
        "best_score = float('inf')\n",
        "best_params = {}\n",
        "\n",
        "for pca_comp in n_components:\n",
        "    for mca_comp in n_components:\n",
        "        # Standardize numerical features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_num_scaled = scaler.fit_transform(X_train_num)\n",
        "        X_valid_num_scaled = scaler.fit_transform(X_valid_num)\n",
        "\n",
        "        # Applying PCA on scaled numerical features\n",
        "        pca = PCA(n_components=pca_comp)\n",
        "        X_train_num_pca = pca.fit_transform(X_train_num_scaled)\n",
        "        X_valid_num_pca = pca.transform(X_valid_num_scaled)\n",
        "        # Keeping the original indexes\n",
        "        X_train_num_pca = pd.DataFrame(data=X_train_num_pca, index=X_train_num.index)\n",
        "        X_valid_num_pca = pd.DataFrame(data=X_valid_num_pca, index=X_valid_num.index)\n",
        "\n",
        "        # Applying MCA on categorical features\n",
        "        mca = MCA(n_components=mca_comp)\n",
        "        X_train_cat_mca = mca.fit_transform(X_train_cat)\n",
        "        X_valid_cat_mca = mca.transform(X_valid_cat)\n",
        "\n",
        "        # Combining the result of PCA and MCA together\n",
        "        X_train_reduced = pd.concat([pd.DataFrame(X_train_num_pca), pd.DataFrame(X_train_cat_mca)], axis=1)\n",
        "        X_valid_reduced = pd.concat([pd.DataFrame(X_valid_num_pca), pd.DataFrame(X_valid_cat_mca)], axis=1)\n",
        "\n",
        "        # Initialize and fit the linear regression model\n",
        "        model = LinearRegression()\n",
        "        model.fit(X_train_reduced, y_train)\n",
        "\n",
        "        y_pred = model.predict(X_valid_reduced)\n",
        "\n",
        "        mse = mean_squared_error(y_valid, y_pred)\n",
        "\n",
        "        # Update the best score and parameters if this model is better\n",
        "        if mse < best_score:\n",
        "            best_score = mse\n",
        "            best_params = {'pca_comp': pca_comp,\n",
        "                           'mca_comp': mca_comp}\n",
        "\n",
        "best_params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### _Performance_\n",
        "\n",
        "After finding the optimal combinations of number of components for both PCA and MCA, we now evaluate the performance of the model using the testing set.\n",
        "\n",
        "First, we have to prepare the data again. Now the training side will be composed of training and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separating categorical and numerical features\n",
        "X_train_val_cat = X_train_val.select_dtypes(include=['object'])\n",
        "X_train_val_num = X_train_val.select_dtypes(include=['int', 'float'])\n",
        "\n",
        "# For valid test\n",
        "X_test_cat = X_test.select_dtypes(include=['object'])\n",
        "X_test_num = X_test.select_dtypes(include=['int', 'float'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, we make sure that each categorical variables in the training set and validation set have the same set of levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validating that the training feature and testing feature have same number of levels\n",
        "keep = X_train_val_cat.nunique() == X_test_cat.nunique()\n",
        "# Removing columns whose levels don't match\n",
        "X_train_val_cat = X_train_val_cat[X_train_val_cat.columns[keep]]\n",
        "X_test_cat = X_test_cat[X_test_cat.columns[keep]]\n",
        "\n",
        "# Making sure the classes are the same for categorical features that have same levels\n",
        "keep = []\n",
        "for i in range(X_train_val_cat.shape[1]):\n",
        "    keep.append(all(np.sort(X_train_val_cat.iloc[:,i].unique()) == np.sort(X_test_cat.iloc[:,i].unique())))\n",
        "X_train_val_cat = X_train_val_cat[X_train_val_cat.columns[keep]]\n",
        "X_test_cat = X_test_cat[X_test_cat.columns[keep]]\n",
        "\n",
        "X_test = pd.concat([X_test_cat, X_valid_num], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using training+validation to fit the model, the using the testing set to predict and evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension Reduction Mean Squared Error: 1030130708.3429468\n"
          ]
        }
      ],
      "source": [
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_val_num_scaled = scaler.fit_transform(X_train_val_num)\n",
        "X_test_num_scaled = scaler.fit_transform(X_test_num)\n",
        "\n",
        "# Applying PCA on scaled numerical features\n",
        "pca = PCA(n_components=best_params['pca_comp'])\n",
        "X_train_val_num_pca = pca.fit_transform(X_train_val_num_scaled)\n",
        "X_test_num_pca = pca.transform(X_test_num_scaled)\n",
        "# Keeping the original indexes\n",
        "X_train_val_num_pca = pd.DataFrame(data=X_train_val_num_pca, index=X_train_val_num.index)\n",
        "X_test_num_pca = pd.DataFrame(data=X_test_num_pca, index=X_test_num.index)\n",
        "\n",
        "# Applying MCA on categorical features\n",
        "mca = MCA(n_components=best_params['mca_comp'])\n",
        "X_train_val_cat_mca = mca.fit_transform(X_train_val_cat)\n",
        "X_test_cat_mca = mca.transform(X_test_cat)\n",
        "\n",
        "# Combining the result of PCA and MCA together\n",
        "X_train_val_reduced = pd.concat([pd.DataFrame(X_train_val_num_pca), pd.DataFrame(X_train_val_cat_mca)], axis=1)\n",
        "X_test_reduced = pd.concat([pd.DataFrame(X_test_num_pca), pd.DataFrame(X_test_cat_mca)], axis=1)\n",
        "\n",
        "# Initialize and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_val_reduced, y_train_val)\n",
        "\n",
        "y_pred = model.predict(X_test_reduced)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Dimension Reduction Mean Squared Error: {mse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ridge Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a baseline model, we will use a Ridge regression model.\n",
        "\n",
        "First, we start with splitting the dataset into training and testing. We concatenate the dataframes containing the numerical and caterogical features together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = pd.concat([X_train_val_num, pd.get_dummies(X_train_val_cat, drop_first=True)], axis=1)\n",
        "X_test = pd.concat([X_test_num, pd.get_dummies(X_test_cat, drop_first=True)], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define the range of values used to tune the hyperparameter `alpha` ($lambda$) for the Ridge regression model.\n",
        "\n",
        "With the model obtained by `GridSearchCV`, we make the predictions on the validation set.\n",
        "\n",
        "Finally, we calculate and print the mean squared error for this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ridge Regression Mean Squared Error: 937115200.3955582\n"
          ]
        }
      ],
      "source": [
        "# Create a Ridge regression model\n",
        "ridge = Ridge()\n",
        "\n",
        "# Define a range of alpha values to tune\n",
        "param_grid = {'alpha': [0.01, 0.1, 1.0, 10.0]}\n",
        "\n",
        "# Create a GridSearchCV object with Ridge regression and alpha parameter grid\n",
        "grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the grid search to the training data\n",
        "grid_search.fit(X_train, y_train_val)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_test_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "ridge_mse = mean_squared_error(y_test, y_test_pred)\n",
        "print(f\"Ridge Regression Mean Squared Error: {ridge_mse}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Model               | MSE                |\n",
        "| ------------------- | ------------------ |\n",
        "| Dimension Reduction | 1030130708.3429468 |\n",
        "| Ridge               | 937115200.3955582  |\n",
        "\n",
        "In summary, based on the MSE values you provided, the tuned Ridge regression model is outperforming the simple linear regression model with PCA and MCA dimension reduction in terms of predictive accuracy, as indicated by the lower MSE.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part II: Clustering Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While we perform dimension reduction separately for numerical and categorical data, there are methods that can perform clustering analysis with numerical and categorical data combined. As usual, the most important aspect is the distance metric to use. For mixed data types, researchers have proposed to use the Gower distance. The Gower distance is essentially a special distance metric that measures numerical data and categorical data separately, then combine them to form a distance calculation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculating Gower Distance Matrix on the full predictors set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "g_matrix = gower.gower_matrix(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "K-Medoids model parameters, in order:\n",
        "1. `g_matrix`: calculated gower distance to feed the K-Medoids model.\n",
        "2. `initial_medoids`: randomized initial medoids values, equivalent to the number of components.\n",
        "3. `'distance_matrix'`: instructing the model that the input value is not raw data but a precalculated distance matrix.\n",
        "\n",
        "Finally, we print the indexes of observations for each cluster. Below we print the indexes of the medoids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0, 6, 13, 18, 22, 25, 27, 32, 34, 36, 45, 47, 53, 60, 64, 67, 81, 82, 89, 96, 100, 103, 112, 117, 118, 123, 133, 141, 143, 148, 151, 162, 178, 186, 189, 192, 196, 199, 200, 203, 211, 213, 216, 219, 220, 224, 229, 235, 237, 239, 250, 256, 275, 277, 280, 281, 282, 283, 290, 301, 304, 308, 325, 331, 332, 335, 336, 337, 342, 354, 361, 377, 380, 384, 387, 388, 398, 399, 400, 411, 419, 420, 425, 427, 439, 442, 443, 456, 460, 464, 467, 471, 472, 473, 475, 477, 479, 480, 482, 495, 506, 514, 524, 528, 537, 538, 540, 550, 551, 557, 565, 579, 583, 591, 593, 595, 596, 602, 604, 608, 610, 611, 616, 629, 637, 638, 640, 642, 651, 661, 664, 667, 670, 674, 679, 680, 685, 687, 696, 697, 701, 703, 704, 709, 717, 718, 721, 723, 724, 727, 739, 741, 745, 749, 762, 765, 771, 772, 773, 779, 780, 787, 790, 802, 808, 812, 814, 816, 821, 822, 824, 835, 847, 848, 861, 863, 876, 877, 882, 885, 895, 903, 919, 920, 922, 925, 927, 929, 930, 931, 943, 954, 959, 976, 981, 988, 993, 996, 998, 1002, 1010, 1013, 1014, 1015, 1017, 1018, 1021, 1027, 1030, 1036, 1038, 1041, 1045, 1050, 1054, 1068, 1072, 1073, 1074, 1076, 1079, 1089, 1100, 1103, 1109, 1111, 1115, 1120, 1121, 1132, 1136, 1140, 1149, 1150, 1151, 1152, 1155, 1159, 1160, 1175, 1181, 1187, 1192, 1197, 1199, 1201, 1203, 1211, 1221, 1222, 1232, 1234, 1235, 1243, 1249, 1257, 1260, 1263, 1272, 1280, 1289, 1295, 1297, 1299, 1302, 1303, 1308, 1309, 1310, 1316, 1319, 1322, 1333, 1339, 1340, 1351, 1353, 1360, 1361, 1365, 1374, 1379, 1381, 1385, 1393, 1394, 1396, 1401, 1404, 1406, 1410, 1413, 1420, 1428, 1432, 1435, 1438, 1442, 1443, 1444, 1445], [1, 5, 8, 9, 15, 21, 24, 29, 48, 61, 63, 74, 77, 79, 93, 104, 108, 110, 115, 120, 121, 125, 127, 130, 132, 149, 155, 163, 164, 165, 170, 177, 181, 182, 184, 187, 198, 202, 204, 217, 238, 241, 245, 262, 266, 270, 274, 278, 288, 291, 295, 303, 306, 307, 311, 313, 322, 324, 334, 344, 351, 353, 357, 359, 360, 364, 370, 379, 385, 386, 389, 393, 397, 401, 405, 406, 417, 426, 430, 435, 436, 438, 441, 447, 448, 449, 455, 458, 462, 466, 487, 493, 496, 501, 507, 519, 521, 527, 529, 533, 541, 544, 545, 546, 553, 555, 560, 561, 573, 574, 575, 577, 582, 584, 594, 597, 599, 605, 612, 613, 615, 628, 633, 636, 641, 645, 648, 650, 654, 656, 666, 673, 678, 690, 691, 695, 700, 708, 713, 720, 726, 732, 737, 746, 747, 752, 755, 764, 781, 783, 785, 796, 798, 804, 811, 823, 836, 838, 841, 845, 849, 871, 883, 884, 887, 910, 912, 914, 916, 918, 924, 932, 935, 938, 941, 951, 952, 962, 964, 972, 979, 980, 989, 1003, 1004, 1016, 1024, 1025, 1028, 1034, 1043, 1053, 1057, 1062, 1063, 1065, 1066, 1069, 1070, 1086, 1087, 1088, 1092, 1108, 1112, 1117, 1123, 1131, 1138, 1139, 1141, 1142, 1143, 1147, 1162, 1165, 1167, 1171, 1172, 1174, 1178, 1179, 1184, 1186, 1191, 1196, 1206, 1207, 1210, 1212, 1216, 1224, 1228, 1229, 1241, 1248, 1255, 1256, 1262, 1266, 1267, 1271, 1276, 1277, 1293, 1300, 1306, 1311, 1318, 1320, 1323, 1324, 1325, 1331, 1335, 1337, 1341, 1344, 1347, 1352, 1354, 1359, 1362, 1369, 1375, 1376, 1378, 1380, 1384, 1388, 1389, 1390, 1391, 1395, 1399, 1402, 1405, 1407, 1418, 1419, 1429, 1431, 1439, 1440, 1449], [2, 3, 4, 11, 20, 35, 46, 50, 56, 57, 58, 62, 65, 72, 84, 85, 86, 87, 94, 95, 101, 105, 111, 114, 119, 131, 138, 139, 145, 147, 157, 158, 159, 161, 167, 168, 169, 172, 176, 180, 183, 185, 190, 193, 197, 208, 212, 218, 221, 222, 226, 231, 236, 243, 247, 248, 251, 254, 255, 257, 260, 265, 269, 271, 279, 284, 289, 292, 296, 300, 305, 309, 314, 315, 316, 317, 319, 320, 321, 323, 327, 333, 339, 347, 348, 349, 350, 358, 363, 369, 373, 375, 376, 378, 381, 383, 390, 402, 403, 407, 408, 413, 414, 416, 422, 432, 434, 445, 446, 451, 452, 457, 459, 468, 476, 481, 491, 500, 505, 510, 516, 517, 518, 522, 523, 526, 532, 534, 539, 542, 543, 547, 552, 556, 562, 564, 566, 570, 571, 576, 581, 588, 589, 598, 600, 601, 606, 614, 617, 621, 622, 626, 639, 649, 658, 662, 675, 681, 682, 683, 684, 686, 688, 689, 699, 705, 711, 715, 728, 729, 734, 736, 742, 743, 744, 748, 750, 753, 756, 759, 760, 761, 766, 778, 784, 786, 789, 791, 795, 797, 799, 800, 817, 819, 826, 828, 829, 843, 846, 854, 856, 857, 865, 866, 868, 872, 874, 878, 879, 899, 900, 904, 906, 911, 915, 917, 923, 926, 933, 934, 937, 944, 955, 960, 961, 967, 970, 975, 977, 982, 983, 984, 985, 986, 987, 1006, 1009, 1012, 1026, 1031, 1037, 1040, 1044, 1048, 1051, 1052, 1059, 1060, 1078, 1081, 1082, 1083, 1085, 1090, 1099, 1101, 1102, 1104, 1110, 1118, 1122, 1127, 1128, 1135, 1146, 1148, 1158, 1161, 1163, 1166, 1168, 1169, 1176, 1182, 1183, 1185, 1189, 1190, 1195, 1204, 1205, 1218, 1220, 1230, 1231, 1233, 1237, 1238, 1239, 1244, 1246, 1247, 1251, 1253, 1258, 1261, 1281, 1290, 1292, 1294, 1296, 1298, 1304, 1305, 1314, 1321, 1330, 1334, 1336, 1345, 1346, 1350, 1355, 1356, 1357, 1358, 1364, 1366, 1367, 1373, 1377, 1386, 1392, 1400, 1408, 1414, 1417, 1421, 1424, 1433, 1446, 1448], [7, 10, 12, 14, 16, 17, 19, 23, 26, 28, 30, 31, 33, 37, 38, 39, 40, 41, 42, 43, 44, 49, 51, 52, 54, 55, 59, 66, 68, 69, 70, 71, 73, 75, 76, 78, 80, 83, 88, 90, 91, 92, 97, 98, 99, 102, 106, 107, 109, 113, 116, 122, 124, 126, 128, 129, 134, 135, 136, 137, 140, 142, 144, 146, 150, 152, 153, 154, 156, 160, 166, 171, 173, 174, 175, 179, 188, 191, 194, 195, 201, 205, 206, 207, 209, 210, 214, 215, 223, 225, 227, 228, 230, 232, 233, 234, 240, 242, 244, 246, 249, 252, 253, 258, 259, 261, 263, 264, 267, 268, 272, 273, 276, 285, 286, 287, 293, 294, 297, 298, 299, 302, 310, 312, 318, 326, 328, 329, 330, 338, 340, 341, 343, 345, 346, 352, 355, 356, 362, 365, 366, 367, 368, 371, 372, 374, 382, 391, 392, 394, 395, 396, 404, 409, 410, 412, 415, 418, 421, 423, 424, 428, 429, 431, 433, 437, 440, 444, 450, 453, 454, 461, 463, 465, 469, 470, 474, 478, 483, 484, 485, 486, 488, 489, 490, 492, 494, 497, 498, 499, 502, 503, 504, 508, 509, 511, 512, 513, 515, 520, 525, 530, 531, 535, 536, 548, 549, 554, 558, 559, 563, 567, 568, 569, 572, 578, 580, 585, 586, 587, 590, 592, 603, 607, 609, 618, 619, 620, 623, 624, 625, 627, 630, 631, 632, 634, 635, 643, 644, 646, 647, 652, 653, 655, 657, 659, 660, 663, 665, 668, 669, 671, 672, 676, 677, 692, 693, 694, 698, 702, 706, 707, 710, 712, 714, 716, 719, 722, 725, 730, 731, 733, 735, 738, 740, 751, 754, 757, 758, 763, 767, 768, 769, 770, 774, 775, 776, 777, 782, 788, 792, 793, 794, 801, 803, 805, 806, 807, 809, 810, 813, 815, 818, 820, 825, 827, 830, 831, 832, 833, 834, 837, 839, 840, 842, 844, 850, 851, 852, 853, 855, 858, 859, 860, 862, 864, 867, 869, 870, 873, 875, 880, 881, 886, 888, 889, 890, 891, 892, 893, 894, 896, 897, 898, 901, 902, 905, 907, 908, 909, 913, 921, 928, 936, 939, 940, 942, 945, 946, 947, 948, 949, 950, 953, 956, 957, 958, 963, 965, 966, 968, 969, 971, 973, 974, 978, 990, 991, 992, 994, 995, 997, 999, 1000, 1001, 1005, 1007, 1008, 1011, 1019, 1020, 1022, 1023, 1029, 1032, 1033, 1035, 1039, 1042, 1046, 1047, 1049, 1055, 1056, 1058, 1061, 1064, 1067, 1071, 1075, 1077, 1080, 1084, 1091, 1093, 1094, 1095, 1096, 1097, 1098, 1105, 1106, 1107, 1113, 1114, 1116, 1119, 1124, 1125, 1126, 1129, 1130, 1133, 1134, 1137, 1144, 1145, 1153, 1154, 1156, 1157, 1164, 1170, 1173, 1177, 1180, 1188, 1193, 1194, 1198, 1200, 1202, 1208, 1209, 1213, 1214, 1215, 1217, 1219, 1223, 1225, 1226, 1227, 1236, 1240, 1242, 1245, 1250, 1252, 1254, 1259, 1264, 1265, 1268, 1269, 1270, 1273, 1274, 1275, 1278, 1279, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1291, 1301, 1307, 1312, 1313, 1315, 1317, 1326, 1327, 1328, 1329, 1332, 1338, 1342, 1343, 1348, 1349, 1363, 1368, 1370, 1371, 1372, 1382, 1383, 1387, 1397, 1398, 1403, 1409, 1411, 1412, 1415, 1416, 1422, 1423, 1425, 1426, 1427, 1430, 1434, 1436, 1437, 1441, 1447, 1450]]\n",
            "[216, 836, 168, 881]\n"
          ]
        }
      ],
      "source": [
        "n_components = 4\n",
        "initial_medoids = np.random.randint(20, size=(1, n_components)).tolist()[0]\n",
        "\n",
        "# Creating and fitting the K-Medoid model\n",
        "kmedoids_instance = kmedoids(g_matrix, initial_medoids, data_type='distance_matrix')\n",
        "kmedoids_instance.process()\n",
        "\n",
        "# Clusters and Medoids\n",
        "clusters = kmedoids_instance.get_clusters()\n",
        "medoids = kmedoids_instance.get_medoids()\n",
        "\n",
        "print(clusters)\n",
        "print(medoids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the clustering result only indicate which observations belong to what cluster $k_i$, we store an observation's cluster number in the list `km_cluster`.\n",
        "\n",
        "Below we bin the response variable into the numbers of categories used for k-Medoids (4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-Medoids assigned cluster to each observation\n",
        "km_cluster = []\n",
        "for n_c, cluster in enumerate(clusters):\n",
        "    for index in cluster:\n",
        "        # lst.append([index, n_c])\n",
        "        km_cluster.append(n_c)\n",
        "    \n",
        "# Ground truth\n",
        "bins = pd.qcut(y, q=n_components, labels=False).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we calculate the Normalized Mutual Information (NMI)  between the K-Medoids clusters and the binning method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0032549711961263605"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "normalized_mutual_info_score(km_cluster, bins)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The obtained NMI means that the clustering obatined by the response variable, SalePrice, is very dissimilar to the one performd by K-Medoids.\n",
        "\n",
        "Since the ground truth used by this analysis is not ruled by true labels on the dataset, we cannot say if K-Medoids or the binning method is a better clustering criteria. We can only conclude that both models capture the relationships between features in a different manner."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
